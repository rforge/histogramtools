\documentclass{article}
%% %\VignetteIndexEntry{HistogramTools-intro}

%% JSS stuff we can remove for JSS version.
\usepackage[authoryear,round,longnamesfirst]{natbib}

\bibpunct{(}{)}{;}{a}{}{,}
\bibliographystyle{jss}
%% page layout
\topmargin 0pt
\textheight 46\baselineskip
\advance\textheight by \topskip
\oddsidemargin 0.1in
\evensidemargin 0.15in
\marginparwidth 1in
\oddsidemargin 0.125in
\evensidemargin 0.125in
\marginparwidth 0.75in
\textwidth 6.125in
%% paragraphs
\setlength{\parskip}{0.7ex plus0.1ex minus0.1ex}
\setlength{\parindent}{0em}

% The \cite command functions as follows:
%   \citet{key} ==>>                Jones et al. (1990)
%   \citet*{key} ==>>               Jones, Baker, and Smith (1990)
%   \citep{key} ==>>                (Jones et al., 1990)
%   \citep*{key} ==>>               (Jones, Baker, and Smith, 1990)
%   \citep[chap. 2]{key} ==>>       (Jones et al., 1990, chap. 2)
%   \citep[e.g.][]{key} ==>>        (e.g. Jones et al., 1990)
%   \citep[e.g.][p. 32]{key} ==>>   (e.g. Jones et al., p. 32)
%   \citeauthor{key} ==>>           Jones et al.
%   \citeauthor*{key} ==>>          Jones, Baker, and Smith
%   \citeyear{key} ==>>             1990

%%
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}
\usepackage{Sweave}
\usepackage{listings}             % Include the listings-package
\usepackage{url}
\usepackage{graphicx}

<<echo=FALSE,print=FALSE>>=
library("HistogramTools")
options("width"=65)
ht.version <- packageDescription("HistogramTools")$Version
prettyDate <- format(Sys.Date(), "%B %e, %Y")
@ 
% closing $ needed here

%% almost as usual
\author{Murray Stokely}
\title{HistogramTools for Distributions of Large Data Sets}
\date{Version \Sexpr{ht.version} as of \Sexpr{prettyDate}}
\begin{document}

\maketitle
  
\abstract{
  \noindent
  Histograms are a common graphical representation of the distribution
  of a data set.  They are particularly useful for collecting very
  large data sets into a binned form for easier data storage and
  analysis.  The \pkg{HistogramTools} R package augments the
  built-in support for histograms with a number of methods that are
  useful for analyzing large data sets.  Specifically, methods are
  included for
  serializing histograms into a compact Protocol Buffer representation
  for sharing between distributed tasks, and routines for
  manipulating the resulting aggregate histograms.
}
% \Keywords{histograms, distributions, R, mapreduce, protocol buffers, RProtoBuf}
% \Plainkeywords{histograms, distributions, R, mapreduce, protocol
%  buffers, RProtoBuf} %% without formatting.
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:

%\Address{
%  Murray Stokely\\
%  Google, Inc.\\
%  1600 Amphitheatre Parkway\\
%  Mountain View, CA 94043\\
%  E-mail: \email{mstokely@google.com}\\
%  URL: \url{http://research.google.com/pubs/MurrayStokely.html}
%  }
%
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Histograms have been used for over a century
\citep{pearson1895contributions} as a compact graphical
representation of a distribution of data.  Support for generating
histograms is included in almost all statistical software, including
R.  However, the growth in large-scale data analysis in R with the
MapReduce \citep{dean2008mapreduce} paradigm has highlighted some gaps
in functionality that this package hopes to fill.

% TODO(mstokely): Also cite the streaming algorithm work?
Much previous work on histograms
\citep{sturges1926choice,scott1979optimal} involves indentifying ideal
breakpoints for a dataset.  In a large MapReduce environment over
large data sets a different sort of engineering challenges comes up.
For example, how do we efficiently store large numbers of histograms
generated frequently by real-time monitoring systems of many thousands
of computers?  How can we aggregate histograms generated by different
tasks in a large scale computation?  If we chose very granular bucket
boundaries for our initial data collection pass, how can we then
reshape the bucket boundaries to be more relevant to the analysis
questions at hand?

% [cite R ?hist algo] but in the parallell
%context each histogram must share the same breakpoints, even if they
%are not the optimal ones for the specific data subset any particular
%routine is accessing.  If we hope to merge the resulitng histograms
%into a meaningful metric of the full data set.


%This paper introduces some extensions to the histogram functionality in R
%\citep{r} to facilitate collection, transmission, manipulation of histograms in% a MapReduce \citep{dean2008mapreduce} environment of large compute clusters.

%Histograms are increasingly used in leage scale distributed systems research.  Especially with the MapReduce paradigm where many thousands of tasks may analyze a big data set and collect distributions of interest  Exampels from search clusters, distributed filesystems, etc. [cite]


\section{Parallel Data Collection Patterns}

Many large datasets in fields such as particle physics and information
processing are stored in binned or histogram form in order to reduce
the data storage requirements \cite{scott2009multivariate}.

There are two common patterns for generating histograms of large data
sets with MapReduce.  In the first method, each mapper task can
generate a histogram over a subset of the data that is has been
assigned, and then the histograms of each mapper are sent to one or
more reducer tasks to merge.

In the second method, each mapper rounds a data point to a bucket
width and outputs that bucket as a key and '1' as a value.  Reducers
then sum up all of the values with the same key and output to a data store.

In both of these two methods, the mapper tasks must choose identical
bucket boundaries even though they are analyzing disjoint parts of the
input set that may be skewed in different ways, or we must implement
multiple phases.

\begin{figure}[h!]
\begin{center}
\includegraphics{histogram-mapreduce-diag1.pdf}
\end{center}
\caption{Diagram of MapReduce Histogram Generation Pattern}
\label{fig:mr-histogram-pattern1}
\end{figure}

Figure~\ref{fig:mr-histogram-pattern1} illustrates the second method
described above for histogram generation of large datasets with
MapReduce.

This package is designed to be helpful if some of the Map or Reduce
tasks are written in R, or if those components are written in other
languages and only the resulting output histograms need to be
manipulated in R.

%There are really two cookbook recipes for generating histograms of large data sets.  Have each mapper generate an actual histogram, in which case the reducer must merge them.

%Or have each mapper output round to a bucket width and output for every bucket width item
%then have the reducer sum those up.

%Why do we do the first one?  Why not the second one?

%What is the runtime difference between the two?  Is the second faster?  Or the first?

%TODO:
%Good diagram for paper showing MapReduce generation of histograms.  As described in Hadoop MapReduce cookbook :

% Histogram makes sense only under a continuous dimension (for example, access time and file % size). It groups the number of occurrences of some event into several groups in the dimension. % For example, in this recipe, if we take the access time from weblogs as the dimension, then we will group the access time by the hour.
%
%The following figure shows a summary of the execution. Here the mapper calculates the hour of %the day and emits the "hour of the day" and 1 as the key and value respectively. Then each %reducer receives all the occurrences of one hour of a day, and calculates the number of %occurrences:
% THIS ABOVE IS FROM :
%\url{http://my.safaribooksonline.com/book/-/9781849517287/6dot-analytics/ch06s06_html}

%\section{Applications}
%Very large datasets in Pearson's day meant more than 100 observations, but analysis of web-scale data, such as the number of unlinks in the trillions of pages on the web requires significantly larger computation resources.
%Pearson was interested in very large data sets (more than 1000) 

%Trimming

%If, say you are only interested in part of a distribution, for example for performance measurements the requests that had a latency of greater than X seconds only.  Then the Trim() functions can help manipulate the histograms to zoom in on an area of interest for additional plotting.


%\section{Protocol Buffer Representations of Histograms}

\section{Efficient Representations of Histograms}

Consider an example histogram of 100 random data points.
Figure~\ref{fig:exhist} shows the graphical representation and list
structure of R histogram objects.

<<exhist,fig=TRUE,include=FALSE,echo=TRUE>>=
myhist <- hist(runif(100))
@

\begin{figure}[h]
\begin{minipage}{.4\textwidth}
\begin{center}
\includegraphics[width=2.5in,height=2.5in]{HistogramTools-exhist}
\end{center}
%\captionof{figure}{Example Histogram}
%\label{fig:exhist}
\end{minipage}\hfill \begin{minipage}{.5\textwidth}
\begin{tiny}
<<echo=FALSE,print=TRUE>>=
myhist
@
\end{tiny}
\end{minipage}
\caption{Example Histogram}
\label{fig:exhist}
\end{figure}

This histogram compactly represents the full distribution.
Histogram objects in R are lists with 6 components: breaks, counts,
density, mids, name, and equidist.

If we are working in a parallel environment and need to distribute
such a histogram to other tasks running in a compute cluster, we will
need to serialize this histogram object to a binary format that can be
transferred over the network.

\subsection{Native R Serialization}

R includes a built-in serialization framework that allows one to
serialize any R object to an Rdata file.

<<echo=TRUE,print=TRUE>>=
length(serialize(myhist, NULL))
@

This works and is quite convenient if the histogram must only be
shared between tasks running the R interpreter, but it is not a very
portable format.

\subsection{Protocol Buffers}

Protocol Buffers are a flexible, efficient, automated, cross-platform
mechanism for serializing structured data \citep{Pike:2005:IDP:1239655.1239658,protobuf}.  The RProtoBuf package
\citep{rprotobuf} provides an interface for manipulating protocol
buffer objects directly within R.

Of the \Sexpr{length(myhist)} elements stored in an R histogram object, we only need to
store three in our serialization format since the others can be
re-computed.  This leads to the following simple protocol buffer
definition of the breaks, counts, and name of a histogram:

<<echo=FALSE>>=
invisible(cat(paste(readLines(system.file("proto/histogram.proto",
                                package="HistogramTools")), "\n")))
@

The package provides \code{as.Message} and \code{as.histogram} methods
for converting between R histogram objects and this protocol buffer
representation.

In addition to the added portability, the protocol buffer
representation is significantly more compact.

<<echo=TRUE,print=TRUE>>=
hist.msg <- as.Message(myhist)
@ 

Our histogram protocol buffer has a human-readable ASCII representation:

<<echo=TRUE>>=
cat(as.character(hist.msg))
@

But it is most useful when serialized to a compact binary representation:

<<echo=TRUE,print=TRUE>>=
length(hist.msg$serialize(NULL))
@

This protocol buffer representation is by default not compressed
however, so we can do better:

<<echo=TRUE>>=
raw.bytes <- memCompress(hist.msg$serialize(NULL), "gzip")
print(length(raw.bytes))
@

We can then send this compressed binary representation of the
histogram over a network or store it to a cross-platform data store
for later analysis by other tools.  If we want to recreate the
original R histogram object from the serialized protocol buffer we can
use the \code{as.histogram} method.

<<echo=TRUE>>=
uncompressed.bytes <- memDecompress(raw.bytes, "gzip")
@ 
<<echo=TRUE,print=TRUE>>=
length(uncompressed.bytes)
new.hist.proto <- HistogramTools.HistogramState$read(uncompressed.bytes)
@

The resulting histogram that has been converted to a protocol buffer,
serialized, compressed, then uncompressed, parsed, and converted back
to a histogram is the same as our original.

<<fig=TRUE,echo=TRUE,height=4,width=8>>=
par(mfrow=c(1,2))
plot(myhist)
plot(as.histogram(new.hist.proto))
@

\section{Quantiles and Cumulative Distribution Functions}

When histograms are used as a binned data storage mechanism to reduce 
Often if our buckets were chosen wisely in advance it may be preferred
to generate an approximate cumulative distribution function from a
histogram.

The Count, ApproxMean, and ApproxQuantile functions are meant to help
with this, but note that they will only be accurate with very granular
histogram buckets.  They would never be appropriate with histogram
buckets chosen by the default algorithm in R.

<<echo=TRUE>>=
hist <- hist(c(1,2,3), breaks=c(0,1,2,3,4,5,6,7,8,9), plot=FALSE)
@ 
<<echo=TRUE,print=TRUE>>=
Count(hist)
ApproxMean(hist)
ApproxQuantile(hist, .5)
ApproxQuantile(hist, c(.05, .95))
@

\section{Error estimates in CDFs approximated from histograms}

How good is our bucketing scheme for the various types of histograms
we are collecting and using to generate approximate CDFs?  What is the
histogram that has the worst error bounds, and should we modify the
binning strategy to reduce those error bounds?

<<echo=FALSE,print=FALSE>>=
PlotCDFWithErrors <- function(x, h) {
  ## Want to make a plot which visually shows the error inherent with
  ## estimating a CDF from a histogram.

  ## The real ECDF line
  plot(ecdf(x))

  yranges <- c(0, cumsum(h$counts)) / sum(h$counts)
  yranges
  rect(head(h$breaks, -1), head(yranges, -1), tail(h$breaks, -1), tail(yranges, -1), col="yellow")
}

PlotAll <- function(x, h) {
  plot(x, main="x")
  plot(h)
  PlotCDFWithErrors(x,h)
}
@

\subsection{Distributions, Histograms, and ECDFs}

A very large data set may need to be binned into histogram form during
the data collection process.  We may then want to convert the
histograms into approximate CDFs for plotting and data analysis.  The
three examples (one in each row) in Figure~\ref{fig:hist2ecdf} display
this process.  The first column represents raw data, the second column
is a histogram, and the third column is an approximate CDF based on
the histograms with yellow boxes representing the error range for the
true CDF based on the original full distribution.  The first row is
simply \code{x <- 1:100}, the second row is \code{runif(100)}, and
the third row is \code{rexp(100)}.

<<hist2ecdf,fig=TRUE,echo=FALSE,print=FALSE,include=FALSE,height=8,width=8>>=
par(mfrow=c(3,3))
x<-1:100
PlotAll(x, hist(x, plot=FALSE))

x<-runif(100)
PlotAll(x, hist(x, plot=FALSE))

x<-rexp(100)
PlotAll(x, hist(x, plot=FALSE))
@

\begin{figure}[h!]
\begin{center}
\includegraphics[width=6in,height=6in]{HistogramTools-hist2ecdf}
\end{center}
\caption{Raw data (column 1), histograms (column 2), approximate ECDFs
  (column 3)}
\label{fig:hist2ecdf}
\end{figure}

The histograms above were generated using the default computed
breakpoints of R, which uses a modified Sturges algorithm
\cite{sturges1926choice}.  However, for our actual data analysis tasks
we generate our own bucket widths in advance so that all of our
histograms have comparable and consistent bucket boundaries.  For the
same distribution, we obviously have control over the histogram bins,
and using more fine-grained bins produces a more accurate
representation of the underlying distribution.  The next figure shows
the same \code{rexp(100)} data set with coarse and fine-grained histogram
buckets and the associated approximate ECDFs.

% lets try some new distributions and histograms
<<fig2,fig=TRUE,echo=TRUE,print=FALSE,height=8,width=8>>=
par(mfrow=c(2,3))
x<-rexp(100)
PlotAll(x, hist(x, plot=FALSE))

PlotAll(x, hist(x, breaks=seq(0,round(max(x) + 1),by=0.1), plot=FALSE))
@

\subsection{Error Estimates}

A natural question is, are we using a good initial choice of buckets?
Which of the 100,000 histograms that we generated last week had the
greatest potential error, and how might we change the bucketing to
reduce that error?

\subsubsection{Kolmogorov-Smirnof}

%Mean Square Error measures the accuracy of an estimator of f in a single point, MISE Mean Integrated Squared Error is the global metric.  But we don't know f for the underlying distribution, since we only have the histogram, so we can't compute these metrics, right?

The yellow boxes on the rightmost ECDF plots show the worst case error
range of the true ECDF of the underlying distribution.  The true CDF
must lie within those yellow boxes, so the widest bound area wise
would be the best?

<<echo=TRUE>>=
SumBoxAreas <- function(x1, y1, x2, y2) {
  return(sum((x2-x1)*(y2-y1)))
}

x<-rexp(100)
h <- hist(x, plot=FALSE)
# convert to density histogram - e.g. normalize to 1.
h$counts <- h$counts / sum(h$counts)
yranges <- c(0, cumsum(h$counts)) / sum(h$counts)
h$breaks <- h$breaks / sum(h$breaks)
SumBoxAreas(head(h$breaks, -1), head(yranges, -1), tail(h$breaks, -1), tail(yranges, -1))

h <- hist(x, breaks=seq(0,round(max(x) + 1),by=0.1), plot=FALSE)
# convert to density histogram - e.g. normalize to 1.
h$counts <- h$counts / sum(h$counts)
yranges <- c(0, cumsum(h$counts)) / sum(h$counts)
h$breaks <- h$breaks / sum(h$breaks)
SumBoxAreas(head(h$breaks, -1), head(yranges, -1), tail(h$breaks, -1), tail(yranges, -1))
@

So, using this metric, the second lower example with more granular
histogram bins produces an ECDF with reduced worst case error bounds
compared to the one with default buckets, as expected.


\section{Histogram Bin Manipulation}

\subsection{Trimming Sparse Buckets}

If we are generating histograms with a large number of fine-grained
bucket boundaries, then we out resulting histogram may have a large
number of empty consecutive buckets on the left or right side of the
histogram.  The \code{TrimHistogram} function can be used to remove
them as illustrated in Figure~\ref{fig:trimhist}.

<<echo=TRUE,print=FALSE>>=
hist.1 <- hist(c(1,2,3), breaks=c(0,1,2,3,4,5,6,7,8,9), plot=FALSE)
hist.trimmed <- TrimHistogram(hist.1)
@ 

<<echo=TRUE,print=TRUE>>=
length(hist.1$counts)
sum(hist.1$counts)
length(hist.trimmed$counts)
sum(hist.trimmed$counts)
@

<<trimhist,fig=TRUE,echo=TRUE,include=FALSE,width=8,height=4>>=
par(mfrow=c(1,2))
plot(hist.1)
plot(TrimHistogram(hist.1))
@

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5in,height=2.5in]{HistogramTools-trimhist}
\end{center}
\caption{Effect of the \code{TrimHistogram} function.}
\label{fig:trimhist}
\end{figure}

\subsection{Merging Histograms}

If two histograms have the same bucket boundaries, it is possible to
merge them together by aggregating the counts values with the \code{merge}
function illustrated in Figure~\ref{fig:mergehist}.

<<echo=TRUE,print=FALSE>>=
hist.1 <- hist(c(1,2,3,4), plot=FALSE)
hist.2 <- hist(c(1,2,2,4), plot=FALSE)
hist.merged <- merge(hist.1, hist.2)
@

<<mergehist,fig=TRUE,echo=FALSE,include=FALSE,width=8,height=4>>=
par(mfrow=c(1,3))
plot(hist.1)
plot(hist.2)
plot(hist.merged,main="Merged Histogram")
@

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5in,height=2.5in]{HistogramTools-mergehist}
\end{center}
\caption{Effect of the \code{MergeHistogram} function.}
\label{fig:mergehist}
\end{figure}

\subsection{Downsampling: Merging Buckets}

If data was binned into too many buckets, it is possible to merge
adjacent buckets with the \code{downsample} function illustrated in Figure~\ref{fig:downsamplehist}.

<<echo=TRUE,print=FALSE>>=
overbinned <- hist(c(rexp(100), 1+rexp(100)), breaks=seq(0,10,by=.01), plot=FALSE)
better.hist <- downsample(overbinned, adj=20)
@

<<downsamplehist,fig=TRUE,echo=FALSE,include=FALSE,width=8,height=4>>=
par(mfrow=c(1,2))
plot(overbinned)
plot(better.hist)
@

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5in,height=2.5in]{HistogramTools-downsamplehist}
\end{center}
\caption{Effect of the \code{downsample} function.}
\label{fig:downsamplehist}
\end{figure}

% TODO
% \section{Extensions}
%\subsection{Exemplar Histograms}
% TODO
%

%Sharing Histograms Between Tools
%In a big data analysis task, we may combine many different tools and languages.  For example, high-performance C++ or Java code might process millions of records to generate histograms and then serialize them (using e.g. Protocol buffers or Thrift) to a data store.
%Other code in Python or R may analyze, merge, clean the resulting outputs.  Finally, a front-end dashboard may read in a JSON representation of the cleaned histograms from the datastore into an interactive Javascript web isualization (sing e.g. D3 (\cite{bostock2011d3}) or Gviz)
%Implementation details:
%
%- Benchamrks of Merge operation
%Benchmarks
%Compare size of R serialized histograms and RProtoBuf serialized histograms.
%%
%code snippet:
%
%Table:  columns:  serialized size, serialized time (ms)
%rows:  R serialize, Rprotobuf naive implementation, RProtobuf C++ optimized implementation


%TODO Table example performance analysis.
% label=tab1,echo=FALSE,results=tex>>=
%require(xtable)
%foo <- data.frame(a=c(1,2,3,4),b=c(5,6,7,8))
%print(xtable(foo, caption="Example Caption", center="centering", file="", label="tab:one",floating=FALSE))
%@
%
%Now we have an example reference to Table\ref{tab:one}.


%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.
\section{Summary}
The HistogramTools package presented in this paper has been in wide use for the last several years to allow engineers to read distribution data from internal data stores written in other languages.  Internal production monitoring tools and databases, as well as the output of many MapReduce jobs have been used.

%\section*{Acknowledgements}
%I would like to thank Marcin Kowalczyk for his work on efficient histogram implementations for performance analysis at Google.
%Fran\c{c}ois Labelle for his improved algorithm XX.

%% Note: If there is markup in \(sub)section, then it has to be escape as above.
\bibliography{refs}

\end{document}
