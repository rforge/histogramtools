\documentclass[preprint]{sig-alternate-per}
\usepackage{epsfig,amsmath,amsfonts}
\usepackage{url, html}

% \usepackage{graphicx} % I didn't have this, but may need it for graphicspath
\graphicspath{{.}{./figures/}}

\begin{document}

\CopyrightYear{2014}

% TODO(unredact): Add page numbers for preprint submitted to help reviewers.
\pagenumbering{arabic}

\title{Polynomial Histograms for Memory-Efficient Representation of
  Long-tailed System Distributions}

\subtitle{Paper \#57, 3 pages}

%TODO(unredact)
%\author{
%  \alignauthor
%  Murray Stokely,
%  Nate Coehlo,
%  Arif Merchant\\
%  \affaddr{Google, Inc.} \\
%  \email{\small\tt\{mstokely,natec,aamerchant\}@google.com}
%}
\maketitle

\begin{abstract}

Distributed systems must frequently keep track of many different types
of performance metrics across
%a complex software stack running on
many
different computers.  For example, the latency distribution of certain
operations may be computed for a large combination of
computers, users, and operations.  These
empirical distributions need to be collected at minimal expense on
the individual software components, efficiently aggregated across
multiple dimensions, and stored in a compact representation for a variety of
downstream data analysis applications.

We describe an information loss metric for binned data that allows us
to optimize cost of information loss from different histogram
representations.
%for
%comparing the binning error introduced by different
%histogram representations.
We explore the use of polynomial
histograms where each bin of a histogram is annotated with moments of
the underlying distribution in that bin.  These polynomial histograms
are compared to traditional histograms using the same storage
cost for additional bins instead of annotations in each bin.  We
describe an application of these techniques for file system metrics
for a large production
% distributed file
system, and analytically
characterize when polynomial histograms offer more
information at lower cost.

%We introduce an information loss metric for binned data that is
%applicable to a number of different applications:
%fitting to parametric distributions, comparing mean/variance of
%different histograms, approximate percentiles, etc.

%We then use this information loss metric to compare the accuracy of
%traditional histograms compared to histograms with half as many
%buckets but annotated with the mean value of each bucket over a number
%of empirical data sets from a large-scale distributed system.

%We show that polynomial histograms perform better than traditional
%histograms for the same memory footprint whenever large numbers of
%points are clustered near the bin boundaries.  We use these findings
%from a real-world application to develop engineering rules of thumb
%about the merits of polynomial histograms versus regular histograms.


%Quantities of interest in distributed systems are often stored as
%histograms in order to reduce the data storage requirements
%\cite{scott2009multivariate}.  However, binning of an empirical

%Compact histogram representations are often used with fixed bin sizes
%chosen in advance and stored separately for a large number of
%histograms in order to simplify code paths in distributed systems,
%simplify aggregation across systems, and concisely represent an
%empirical distribution in a limited memory footprint.

%We introduce an information loss metric for binned data that is
%applicable to a number of different applications:
%fitting to parametric distributions, comparing mean/variance of
%different histograms, approximate percentiles, etc.

%We then use this information loss metric to compare the accuracy of
%traditional histograms compared to histograms with half as many
%buckets but annotated with the mean value of each bucket over a number
%of empirical data sets from a large-scale distributed system.

%We show that polynomial histograms perform better than traditional
%histograms for the same memory footprint whenever large numbers of
%points are clustered near the bin boundaries.  We use these findings
%from a real-world application to develop engineering rules of thumb
%about the merits of polynomial histograms versus regular histograms.

\end{abstract}

\section{Introduction}

In many cloud scale systems, monitoring, measuring, and logging
performance metrics is extremely difficult because there are literally
billions of possibly interesting metrics. For example, in a large
distributed file system, we may wish to monitor the resource usage,
throughput, and latency per active user so that we can track down
sources of performance anomalies~\cite{tailscale}. To detect
interactions with other applications and further narrow down the
source of performance issues, we may wish to monitor such metrics for
combinations of users and resources.  However, the memory requirements
for comprehensive logging at this scale are usually exorbitant. An
alternative is to maintain aggregate statistics, such as the empirical
distributions of the metrics, as histograms. Even so, the memory
requirements for maintaining histograms for a large number of metrics
can be burdensome; as such, it is critical to make the histograms as
efficient as possible to minimize information loss while limiting the
memory used.

%\todo{Enough handwaving already - need a brief, concrete example.}
%{\bf We need an example here. I don't know if this is a very good
%  example, but it has the advantage of already being published.Need to
%fill in with some real numbers, rather than hand-wavy ``millions'',
%and corresponding storage requirements.}
% Yes, murray likes what Arif wrote here, though we may need to look
% for sentences to remove or make shorter.

In cloud data centers, it is common to collect histograms per user,
per server, for a variety of metrics - IO delays, network latencies,
CPU throttling, etc. - so that Service Level Agreements can be
monitored and resources allocated appropriately.  For example,
\textbf{Blinded System}
%TODO(unredact)
%Google's Janus system for flash provisioning~\cite{janus}
collects histograms for the ages of files read and also the age of files
stored, per workload group, which can be individual users, subsets of
files from each user, or even groups of columns from a user's
tables. Since there can be tens of thousands of servers, thousands of
users, and tens of metrics being monitored per user/server, there can
be billions of histograms maintained in a datacenter.

Memory requirements for histograms can be reduced in several ways.
We can use coarser bins, but this reduces the fidelity of the histogram. We
could dynamically adjust bin boundaries to improve accuracy, but
this requires additional processing on sensitive nodes, may introduce
non-deterministic overhead, and makes aggregation difficult or impossible across
computers.

%We can combine parametric curve fitting of the individual bins.
%Fitting the histogram bins to the distribution is possible for
%empirical distributions with a large number of samples, but this is
%not possible for distributions with a small number of samples, and the
%resulting irregular bins are harder to combine with other
%distributions.

We explore the effectiveness of using {\em polynomial
  histograms,} where the number of bins is reduced, but the
distribution of samples within each bin is maintained using a
low-order polynomial. For a fixed amount of memory, when is it
preferable to store polynomial annotations in coarser bins?

Our contributions are as follows: (1) We compare the information loss
due to normal (fixed bin) histograms to those with a moment annotation;
(2) we compare the errors empirically for some empirical distributions of system metrics
in a cloud environment; and (3) we give rules of thumb for when
using polynomial histograms is effective.

%Large-scale distributed systems must keep track of many
%distributions.  Such as latency distributions for measuring
%performance against service-level-objectives \cite{tailscale}.

\section{Background}

A variety of techniques have been developed to make synopses of
massive data sets.  Streaming quantile algorithms
\cite{chambers2006monitoring} keep an approximation of a given
quantile of the observed values in a stream.  These algorithms are
most useful when there are a small number of quantiles of interest,
but they do not offer a density estimate across the full distribution
for cases where a variety of downstream data analysis will be done
based on the synopsis.

There has been a lot of work in the database community on histograms
that dynamically adjust bin breakpoints as new data are seen to
minimize error, but these methods are less useful for distributions
with a small number of samples, and the resulting irregular bins are
harder to combine as part of a distributed computation.

Information loss metrics for fixed-boundary histograms of file system parameters are
explored in \cite{douceur1999large}. We utilize similar information
loss metrics but also consider the space versus information loss
tradeoff of adding additional moments to each bin of the histogram to
build low-order ``Polynomial'' \cite{sagae1997bin} or ``Spline''
\cite{Poosala:1997:HET:269157} histograms.


%Fitting the histogram bins to the distribution is possible for
%empirical distributions with a large number of samples, but this is
%not possible for distributions with a small number of samples, and the
%resulting irregular bins are harder to combine with other
%distributions.



%TODO(mstokely): add references about other data synopsis techniques,
%streaming quantile algorithms and the like, as an alternate approach
%to maintaining histograms if you know exactly what you want in
%advance.  Probably that goes at the beginning before getting into
%histograms based on how I started these first two paragraphs.

As with the work of K{\"o}nig and Weikum \cite{konig1999combining} we
do not require continuity across bucket boundries, and
find that this attribute is essential in order to accurately capture
large jumps in bucket frequencies.  Unlike that work, however, we are
not focused on optimal dynamic partitioning of bucket boundaries and
our information loss metric is focused around making definitive statements.
% for query optimization in an online database system.
Instead, we are
focused on constrained resource environments where the computational
and memory requirements of those techniques would be excessive.
% TODO(mstokely): the point about simple accumulators is useful to
% reintroduce somewhere in text though.
%--
% No space, and we've kind of already said this.
%Therefore we seek to understand the tradeoff between using a fixed
%amount of memory for additional histogram bins, or annotating
%histogram bins with additional moments that can be tracked via simple
%accumulators such as the sum of values, sum of squares, and sum of
%cubes for the first three moments.  This allows us to discard the
%actual data values as soon as they are added to the histogram.

% More extensive histogram related background here.  Followup work.
%Ph.D. thesis on this topic from Scott's students, etc.

% TODO: More sophisticated variants exist when all of the data points are kept
%and we can eg. fit splines to the points in each bucket.  We focus
%instead here on the constrained memory situation where we see the data
%stream and record aggregate metrics but do not maintain the full data
%set of each bin.


\subsection{Information Loss Due to Binning}

% NOTE(mstokely): no space for this sentence, we already said something similar
% in background section.
%Quantities of interest in distributed systems are often stored as
%histograms in order to reduce the data storage requirements
%\cite{scott2009multivariate}.
Binning of an empirical
distribution into a histogram representation introduces a form of
preprocessing that constrains all later analyses based on that data
\cite{blocker2013potential}.  Bin breakpoints are often fixed in
advance for specific system quantities to reduce the computational
overhead of keeping track of many different histograms.
However, bin breakpoints that are
poorly chosen relative to the underlying data set may introduce
considerable error when one tries to compute means or percentiles
based on the histogram representations.  This is especially true for
the exponentially bucketed (e.g. buckets that double in size)
representations of distributions such as
latencies or arrival times that have a large dynamic range.

% NOTE(mstokely): No space for this second mention of Janus.  It's
% good text, but I don't see it as more important than the proof or
% empirical results and something has to give.  Arif wrote this.  We
% should add it back for longer write-up.

%
% NOTE(mstokely): Raghav correctly points out we may want to talk
% about cacheability curves a bit here.  They are not mentioned when
% we uncomment this we may need to explain them more in a larger writeup.

%Depending upon the application, it can be critical to bound the error
%introduced by binning. For example, in Janus~\cite{janus},  the authors
%found that the
%cacheability curves for some users were unreliable because they were
%based on relatively few samples. Since such curves may change
%frequently, they introduced instability into the flash allocations,
%and they
% compensated by using longer aggregation periods. Minimizing and
%bounding the binning errors is important when making statistical
%inferences based on histograms.
In evaluating representations of system distributions, we define the
\emph{Earth Mover's Distance of the Cumulative Curves} (EMDCC) as our information
loss metric.  In particular, if $X$ is the (unknown) underlying data set with
distribution $F$, and $h$ is our data representation, r is the range of our representation,
then we define upper and lower bounds $F_{h+}(x) \quad F_{h-}(x)$ as the highest and lowest possible
values for the true distribution given the observed representation, and the
EMDCC as the normalized L1 distance between them:

$$ EMDCC(X, h) = \frac{1}{r} \int_{\mathbb{R}} |F_{h+}(x) - F_{h-}(x)|dx $$

\noindent noting that in the case that $h$ is a histogram bucketing scheme,
$F_{h-}$ always puts its mass on the left endpoints and $F_{h+}$ always puts
near the right endpoints.  The Earth Mover's Distance
\cite{rubner2000earth} is also known as the
Mallows distance, or Wasserstein distance with $p=1$ in
different communities.

Figure~\ref{fig:emdcc} shows a histogram (left) along with the CDF representation and the
associated area of uncertainty (in yellow).  Any underlying dataset
having the given histogram representation must have a true ecdf lying
entirely within the yellow area.  A histogram with more granular
buckets would reduce the information loss at the expense of additional
storage space to store the buckets.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{exhists-crop.pdf}
\caption{An example histogram (left) with its CDF representation and a
  yellow area of uncertainty showing where the true empirical cdf of
  the unbinned data must lie (right).}
\label{fig:emdcc}
\end{figure}

% We say this where we need it in emprical validation section, no
% space to say it twice.
%We assume bucket boundaries are stored separately
%so that histograms are represented as simply an array of integers representing
%the counts of each bucket.
Adding more buckets as in this example usually reduces the EMDCC, but
are there more efficient ways to reduce the EMDCC for a given amount
of storage space?

\subsection{Polynomial Histograms}

Given a fixed amount of storage space, we can trade off the granularity of
histogram buckets for additional statistics within each bucket.  For example,
in addition to storing the counts between histogram boundaries $(a, b)$, we
could also store the the mean and higher moments.  Histograms with
annotations of moments per bin are known as \emph{Polynomial
Histograms} \cite{sagae1997bin}.  Storing the moments is appealing in
a distributed systems context because merging histograms with the same
bucket boundaries remains trivial.  We will use the notation $H(b,p)$ to
denote a histogram with $b$ bins annotated with the $p$-moments of
each bin.

Knowing the first moment can help a lot when it is near the boundary; the
EMDCC associated with bucket $(a, b)$ will be zero if the mean is $a$.
%More generally, assume that we track the $p^{th}$ moment within each bucket $(a,b)$, where
%$$ \mu_p = (\frac{1}{N} \sum_{i=1}^N x_i^p)^{\frac{1}{p}} $$
%Then letting $\alpha = \frac{b^p - \mu_p^2}{b^p - a^p}$ implies that the EMDCC will decrease
%from it's original area by a factor of:
In general, with many points in bucket, a continuous approximation says that a mean of $\mu = \alpha * a + (1-\alpha) * b$ gives an EMDCC of

$$
\lambda(\alpha) = \alpha * ln(\frac{1}{\alpha}) + (1 - \alpha) * ln(\frac{1}{1-\alpha})
$$

The function $\lambda(\alpha)$ is symmetric around $0.5$, is increasing up to it's max of $\lambda(0.5) \approx 0.7$, integrates to $0.5$, and $\lambda(0.2) = \lambda(0.8) \approx 0.5$.  Since bisection always halves the EMDCC, this gives rules of thumb about the merits of bisection vs. storing the mean; if $\alpha < 0.2$ or $\alpha > 0.8$ then storing the mean is better, storing the mean can be worse than bisection by 40\% but it can also be infinitely better, if $\alpha's$ are uniformly distributed and independent of the counts per bucket then bisection and storing the mean should give the same reduction on average.  If the true density is smooth enough relative to the bucketing scheme, then $\alpha$ will tend to be closer to $\frac{1}{2}$, which implies inferiority of keeping the mean with respect to the EMDCC metric.


\noindent
\textbf{Proof:} \\

We restrict our attention to $(a,b)$ where we also know the $p^{th}$ moment$\big(\mu_p^p = \frac{1}{n} \sum_{i=1}^n x_i^p \big)$.  We construct $F_{h+}(x), F_{h-}(x)$ pointwise as the upper and lower bound curves, then integrate to find the reduction in EMDCC. % (only maybe use)  For convenience, we define $\alpha_p = \frac{b^p-\mu_p^p}{b^p-a^p}$

For $x \in (a, \mu_p)$, the lower bound is $F(a)$ and the upper bound has support $\{x, b\}$.  This implies that $\mu_p^p$ equals

$$
\frac{F_{h+}(x)-F(a)}{F(b)-F(a)} * x^p + \frac{F(b)-F_{h+}(x)}{F(b)-F(a)} * b^p
$$

Therefore,

$$
F_{h+}(x) = F(a) + (F(b)-F(a)) \frac{b^p-\mu^p}{b^p-x^p}
$$

For $x \in (\mu_p, b)$, the upper bound is $F(b)$ and the lower bound has support $\{a+\epsilon, x+\epsilon\}$ where we let $\epsilon \rightarrow 0$.  This implies that $\mu_p^p$ equals

$$
\frac{F_{h-}(a+\epsilon)-F(a)}{F(b)-F(a)} * (a+\epsilon)^p + \frac{F_{h-}(x+\epsilon)-F_{h-}(a + \epsilon)}{F(b)-F(a)} (x + \epsilon)^p
$$

Therefore, re-arranging, noting that $F_{h-}(a+\epsilon)=F_{h-}(x)$, $F_{h-}(x+\epsilon)=F(b)$, and letting $\epsilon \rightarrow 0$ gives

$$
F_{h-}(x) = F(b) - (F(b)-F(a)) \frac{\mu_p^p - a^p}{x^p-a^p}
$$

Next, the area reduction from knowing the moment comes from the integral between upper and lower bounds:

$$
\frac{1}{(F(b)-F(a))(b-a)}\int_{a}^{b} |F_{h+}(x) - F_{h-}(x)|dx =
$$
$$
\frac{1}{b-a} \bigg(
\int_{a}^{\mu_p} \frac{b^p-\mu_p^p}{b^p-x^p}dx + \int_{\mu_p}^{b} \frac{\mu_p^p-a^p}{x^p-a^p}dx
\bigg)
$$

\noindent
and this gives the stated result when $p=1$.  More complex formulas exist when multiple moments are known simultaneously.

Figure~\ref{fig:polyemdcc} uses an example bin with points taken from
Beta(0.5, 0.05) and a mean value of 0.9 to illustrate visually the
tradeoff in information loss between $H(2,0)$ and $H(1,1)$ histograms.
Knowing the mean value in this case
constrains the area where an ecdf of the underlying distribution with
that binned representation lies more than if we had just added twice
as many bins at the same storage cost.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{polyhistemdcc-crop.pdf}
\caption{Yellow areas of uncertainty for where the ecdf of the
  unbinned data must lie given a histogram bin bisected in two (left)
  or a histogram annotated with the mean of values in that bin (right).}
\label{fig:polyemdcc}
\end{figure}



\section{Empirical Validation}

We test the efficacy of polynomial histograms in tracking read sizes
for 315 storage users in one of
% TODO(unredact)
\textbf{Blinded's}
% Google's
data centers.  For this
system, we are interested in log read sizes, and we restrict our range from $log(0) = 1 \text{byte}$ to $log(24) = 16\text{MB}$ and find that storing mean and counts in each of the 24 buckets is far more effective than bisecting into 48 buckets.

If we do not store the mean, then K equally sized bins will give an EMDCC of 1/K.  When we store the mean in K equally sized bins and get an EMDCC of X, then we define
$$ \text{information gain} \quad = \frac{1}{2*K*X} $$
A value of 5 implies we would need 5 times as much storage space from equally spaced buckets to achieve the the same EMDCC.  This is bounded below by 1/1.4=0.73, but can get arbitrarily large.

Figure ~\ref{fig:validation} shows the information gain associated with 24 integer buckets of log file sizes, where we have truncated gains above 10.  While $\approx20\%$ see a minor loss, approximately the same number see gains over 10, and 40\% see gains over 2.5.  This shows that 24 buckets with means is superior to 48 regular buckets, and we have also checked that 12 buckets with means is superior to 24 regular buckets, although the relative gain is slightly smaller.  On the other hand, 6 buckets with means is slightly worse 12 regular buckets, because the biggest discontinuities are less likely to sit on the endpoints at this scale.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{inforatio.pdf}
\caption{The Information Gain from storing the mean in 24 integer buckets of log file sizes across 315 storage users.}
\label{fig:validation}
\end{figure}

%Figure~\ref{fig:emdccextreme} shows the file size histograms for two
%different user with two different histogram types.  The first row
%represents user A and the second row represents user B.  The first
%column represents a regular histogram with 52 buckets, and the second
%column represents a polynomial histogram of order 1 with 26 buckets
%and an annotation for the mean in each one.  The first user (row)
%represents a user where the polynomial histogram introduces
%significantly more information loss due to the less granular buckets
%merging in the 0 bucket.  The second user (row) represents the
%opposite extreme where taking half as many buckets doesn't greatly
%increase our information loss, and furthermore the points in the bins
%are near bin boundaries such that the mean provides significant
%additional information.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=\linewidth]{extreme-emdcc-hists-crop.pdf}
%\caption{Two example histograms, with their CDF representation and a
%  yellow area of uncertainty showing where the true empirical cdf of
%  the unbinned data must lie.}
%\label{fig:emdccextreme}
%\end{figure}

\section{Conclusion}
\label{sec:Conclusions}

Cloud data centers monitor a very large number of metric distributions,
particularly for latency metrics, as compact histograms. Memory for
these histograms is limited, and so it is important to use a
representation that minimizes information loss without increasing the
memory footprint. We describe an information loss metric for
histograms, and show that, by using histograms with fewer bins but
adding information about the moments of the samples in the bin,
information loss can be reduced for certain classes of distributions,
and that such distributions occur commonly in practice.
%\section{Availability}
An open-source R package for analyzing the information loss due to
% TODO(unredact)
binning of histogram representations is available at \textbf{Blinded}.
%\cite{histogramtoolsredacted}.
The package includes example code to
generate all figures included in this paper, and the data set
used in the empirical validation section.

% in production systems.
%\section{Acknowledgments}
%
%A polite author always includes acknowledgments.  Thank everyone,
%especially those who funded the work.

%\section{Appendix}

{\footnotesize \bibliographystyle{abbrv}
\bibliography{refs}}

%\theendnotes

\end{document}
